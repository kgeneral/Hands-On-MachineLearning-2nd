{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "19_training_and_deploying_at_scale.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sOPKEqTHZMWy",
        "q39-6axfXcmp",
        "UtdpcXHxQfCF",
        "q2Ydj78Qya99"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgeneral/Hands-On-MachineLearning-2nd/blob/master/19_training_and_deploying_at_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95kltqRQXck7",
        "colab_type": "text"
      },
      "source": [
        "**Chapter 19 – Training and Deploying TensorFlow Models at Scale**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZwc9ginXck-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "90230a8c-278e-4dfc-927b-40a8551201a6"
      },
      "source": [
        "# 기본 설정\n",
        "\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" > /etc/apt/sources.list.d/tensorflow-serving.list\n",
        "    !curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "    !apt update && apt-get install -y tensorflow-model-server\n",
        "    !pip install -q -U tensorflow-serving-api\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deploy\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2943  100  2943    0     0   191k      0 --:--:-- --:--:-- --:--:--  191k\n",
            "OK\n",
            "Hit:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tensorflow-model-server is already the newest version (2.2.0).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm-n4TAs6DaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "eb8d61b7-aaa7-4fc1-9d86-966462cadb75"
      },
      "source": [
        "!pip install --upgrade google-api-python-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.17.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.4)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-api-core<2dev,>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.22.0)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.16.0->google-api-python-client) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtGnMVzkbWCE",
        "colab_type": "text"
      },
      "source": [
        "# 텐서플로 모델 서빙\n",
        "\n",
        "## Tensorflow Serving \n",
        "\n",
        "- TF 모델의 추론결과값을 효율적으로 서비스화(서빙) 하기 위한 프레임워크\n",
        "- 자체 하드웨어 또는 GCP AI 등에서 사용 가능\n",
        "- REST API 또는 gRPC API 사용 사능\n",
        "- **SavedModel 포맷으로 저장된 모델을 사용함**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNyiqd74esFY",
        "colab_type": "text"
      },
      "source": [
        "![TFS](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00175.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdeJc7DOXclC",
        "colab_type": "text"
      },
      "source": [
        "### Save/Load a `SavedModel`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXi7aijMXclC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_new = X_test[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ed2BYFXclF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4f00f50c-4814-4568-c79e-1d87ff964a7f"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7012 - accuracy: 0.8241 - val_loss: 0.3715 - val_accuracy: 0.9024\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3536 - accuracy: 0.9020 - val_loss: 0.2990 - val_accuracy: 0.9144\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3036 - accuracy: 0.9145 - val_loss: 0.2651 - val_accuracy: 0.9272\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2736 - accuracy: 0.9231 - val_loss: 0.2436 - val_accuracy: 0.9334\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2509 - accuracy: 0.9296 - val_loss: 0.2257 - val_accuracy: 0.9364\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2322 - accuracy: 0.9350 - val_loss: 0.2121 - val_accuracy: 0.9396\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2161 - accuracy: 0.9401 - val_loss: 0.1970 - val_accuracy: 0.9454\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2021 - accuracy: 0.9432 - val_loss: 0.1880 - val_accuracy: 0.9476\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1898 - accuracy: 0.9471 - val_loss: 0.1778 - val_accuracy: 0.9524\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1793 - accuracy: 0.9493 - val_loss: 0.1685 - val_accuracy: 0.9540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78a008de10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKYj93TLXclJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "26f72449-9f8f-4bd5-f0e4-e4d56ed999a8"
      },
      "source": [
        "np.round(model.predict(X_new), 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzFXMTn6XclM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66e2df4a-b10f-4ccb-ac26-757283ee440b"
      },
      "source": [
        "model_version = \"0001\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "model_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my_mnist_model/0001'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvPT1YXgXclO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf {model_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT0YIc38XclR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "02c70bd4-64b3-431d-8322-629de8a5d0bd"
      },
      "source": [
        "tf.saved_model.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uQI8i0SfgtB",
        "colab_type": "text"
      },
      "source": [
        "### SavedModel 구조\n",
        "\n",
        "- saved_model.pb : 모델의 computation graph\n",
        "- variables : 모델의 variable\n",
        "- assets : 모델에 필요한 임의 파일(어휘 사전 등) 을 넣을 수 있음\n",
        "  - https://www.tensorflow.org/api_docs/python/tf/saved_model/Asset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1Dlh8sXclU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "55e942c8-6cf5-429b-b0bf-b3e4bb165fc4"
      },
      "source": [
        "for root, dirs, files in os.walk(model_name):\n",
        "    indent = '    ' * root.count(os.sep)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    for filename in files:\n",
        "        print('{}{}'.format(indent + '    ', filename))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_mnist_model/\n",
            "    0001/\n",
            "        saved_model.pb\n",
            "        variables/\n",
            "            variables.data-00001-of-00002\n",
            "            variables.index\n",
            "            variables.data-00000-of-00002\n",
            "        assets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNWE3lc3XclW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e8bccc05-b5a9-40db-b511-b9c8fd20c724"
      },
      "source": [
        "!saved_model_cli show --dir {model_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel contains the following tag-sets:\n",
            "serve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOI9JoVJXclY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6870c4c2-cf1f-4428-a9f7-1587688648c8"
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
            "SignatureDef key: \"__saved_model_init_op\"\n",
            "SignatureDef key: \"serving_default\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqx--GJ6Xclb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "f115b742-bcf5-458c-fb6d-8d9de5eb43c7"
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve \\\n",
        "                      --signature_def serving_default"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['flatten_input'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 28, 28, 1)\n",
            "      name: serving_default_flatten_input:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['dense_1'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 10)\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhvQhuV2Xcle",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67496871-feb7-44a7-dbb1-c9cc791c9395"
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['flatten_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_flatten_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense_1'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0802 07:26:20.064532 140579382863744 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "\n",
            "Defined Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfs0beqWXclg",
        "colab_type": "text"
      },
      "source": [
        "Let's write the new instances to a `npy` file so we can pass them easily to our model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1k5-oRpktKU",
        "colab_type": "text"
      },
      "source": [
        "### numpy 파일의 cli inference\n",
        "\n",
        "- saved_model_cli 에 npy 파일을 입력하여 inference 할 수 있다. (배치성 데이터의 프로세싱 등)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxYzQRzyXclh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inference 할 feature data를 저장\n",
        "np.save(\"my_mnist_tests.npy\", X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4QEMAGQXclj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7dc1322-39f9-4583-f539-b51feb09e030"
      },
      "source": [
        "input_name = model.input_names[0]\n",
        "input_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'flatten_input'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59hWhz2Xclm",
        "colab_type": "text"
      },
      "source": [
        "And now let's use `saved_model_cli` to make predictions for the instances we just saved:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBJ0eYVfXclm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "outputId": "02173ea2-74f3-492e-8430-13a72f90f48c"
      },
      "source": [
        "!saved_model_cli run --dir {model_path} --tag_set serve \\\n",
        "                     --signature_def serving_default    \\\n",
        "                     --inputs {input_name}=my_mnist_tests.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 07:26:23.364521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-02 07:26:23.366973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.367883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-02 07:26:23.368154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-02 07:26:23.370201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-02 07:26:23.372098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-02 07:26:23.372585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-02 07:26:23.374526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-02 07:26:23.375844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-02 07:26:23.380051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-02 07:26:23.380211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.381152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.382098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2020-08-02 07:26:23.387738: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-08-02 07:26:23.388040: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d4d1c08840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-02 07:26:23.388075: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-02 07:26:23.475313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.476364: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d4d1c08a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-02 07:26:23.476411: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-08-02 07:26:23.476638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.477390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-02 07:26:23.477498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-02 07:26:23.477530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-02 07:26:23.477555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-02 07:26:23.477576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-02 07:26:23.477595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-02 07:26:23.477616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-02 07:26:23.477638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-02 07:26:23.477740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.478603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.479372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2020-08-02 07:26:23.479477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-02 07:26:23.480749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-02 07:26:23.480793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
            "2020-08-02 07:26:23.480802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
            "2020-08-02 07:26:23.480962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.482023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-02 07:26:23.482936: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-02 07:26:23.482997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14856 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0802 07:26:23.483913 140318172686208 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/tools/saved_model_cli.py:420: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "2020-08-02 07:26:23.556023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "Result for output key dense_1:\n",
            "[[1.1425152e-04 1.5136739e-07 9.8183926e-04 2.7773660e-03 3.7589029e-06\n",
            "  7.6267330e-05 3.9139668e-08 9.9556208e-01 5.3445601e-05 4.3088742e-04]\n",
            " [8.1954233e-04 3.5499823e-05 9.8824149e-01 7.0576807e-03 1.2937895e-07\n",
            "  2.3403263e-04 2.5744683e-03 9.6686659e-10 1.0369751e-03 8.8340762e-08]\n",
            " [4.4414541e-05 9.7032869e-01 9.0444311e-03 2.2598954e-03 4.8671308e-04\n",
            "  2.8736263e-03 2.2682750e-03 8.3548464e-03 4.0413071e-03 2.9782121e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yIYHsMBXclp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8986dae1-ecd8-4bfe-d74d-4f0883dd3956"
      },
      "source": [
        "np.round([[1.1739199e-04, 1.1239604e-07, 6.0210604e-04, 2.0804715e-03, 2.5779348e-06,\n",
        "           6.4079795e-05, 2.7411186e-08, 9.9669880e-01, 3.9654213e-05, 3.9471846e-04],\n",
        "          [1.2294615e-03, 2.9207937e-05, 9.8599273e-01, 9.6755642e-03, 8.8930705e-08,\n",
        "           2.9156188e-04, 1.5831805e-03, 1.1311053e-09, 1.1980456e-03, 1.1113169e-07],\n",
        "          [6.4066830e-05, 9.6359509e-01, 9.0598064e-03, 2.9872139e-03, 5.9552520e-04,\n",
        "           3.7478798e-03, 2.5074568e-03, 1.1462728e-02, 5.5553433e-03, 4.2495009e-04]], 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.96, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWgv26MyXclr",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow Serving with Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AydGuY8Xcls",
        "colab_type": "text"
      },
      "source": [
        "### docker 를 이용하는 설치 방법\n",
        "- https://docs.docker.com/get-started/part2/\n",
        "    - tensorflow/serving 이미지를 docker hub 에서 다운로드 받아서 \n",
        "    - $ML_PATH/my_mnist_model 로컬 위치의 모델을 docker 이미지 내부의 /models/my_mnist_model 경로로 바인딩하고\n",
        "    - docker 이미지 os 내부에 MODEL_NAME=my_mnist_model 환경 변수를 지정하고\n",
        "    - 8501 포트를 사용해 로컬 머신 에서 띄우는 커맨드\n",
        "```bash\n",
        "docker pull tensorflow/serving\n",
        "export ML_PATH=$HOME/ml # or wherever this project is\n",
        "docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
        "   -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n",
        "   -e MODEL_NAME=my_mnist_model \\\n",
        "   tensorflow/serving\n",
        "```\n",
        "Once you are finished using it, press Ctrl-C to shut down the server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOgSWFYwXcls",
        "colab_type": "text"
      },
      "source": [
        "colab 환경 에서는 `tensorflow_model_server` 를 커스텀 설치 하여 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc6t6HAyXclt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"MODEL_DIR\"] = os.path.split(os.path.abspath(model_path))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6xTPOfXclv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bd410dd-2a3f-42c0-bbfc-e12d9f450894"
      },
      "source": [
        "%%bash --bg\n",
        "nohup tensorflow_model_server \\\n",
        "     --rest_api_port=8501 \\\n",
        "     --model_name=my_mnist_model \\\n",
        "     --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSxqGp65Xclx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "706672dd-6872-4b24-bd2a-df8896afd63a"
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 07:26:25.557338: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /content/my_mnist_model/0001\n",
            "2020-08-02 07:26:25.562282: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:364] SavedModel load for tags { serve }; Status: success: OK. Took 37666 microseconds.\n",
            "2020-08-02 07:26:25.562816: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /content/my_mnist_model/0001/assets.extra/tf_serving_warmup_requests\n",
            "2020-08-02 07:26:25.562933: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: my_mnist_model version: 1}\n",
            "2020-08-02 07:26:25.564235: I tensorflow_serving/model_servers/server.cc:355] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 223] NET_LOG: Couldn't bind to port 8501\n",
            "[evhttp_server.cc : 63] NET_LOG: Server has not been terminated. Force termination now.\n",
            "[evhttp_server.cc : 258] NET_LOG: Server is not running ...\n",
            "2020-08-02 07:26:25.566331: E tensorflow_serving/model_servers/server.cc:377] Failed to start HTTP Server at localhost:8501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQ_uTZSXclz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "input_data_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_new.tolist(),\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh_MbpK0Xcl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7a06c0b4-c700-4a87-bc18-e6f1988b9981"
      },
      "source": [
        "repr(input_data_json)[:1500] + \"...\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\'{\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.32941177487373...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGpJVo_kXcl5",
        "colab_type": "text"
      },
      "source": [
        "### REST API 를 이용한 호출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-HHiZ7sXcl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status() # raise an exception in case of error\n",
        "response = response.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okk92ntwXcl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a348a7fa-214b-4105-9ffa-1a68dd2c5c7a"
      },
      "source": [
        "response.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['predictions'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c_DlYneXcl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f999a378-b641-4f22-cb86-57d1f0d20174"
      },
      "source": [
        "y_proba = np.array(response[\"predictions\"])\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oOWbym8XcmA",
        "colab_type": "text"
      },
      "source": [
        "### gRPC API 를 이용한 호출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjqtgfksXcmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "input_name = model.input_names[0]\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stxwrdYTXcmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "\n",
        "channel = grpc.insecure_channel('localhost:8500')\n",
        "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "response = predict_service.Predict(request, timeout=10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro5Pq4n8XcmF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "0de58cc8-87dc-44ee-9659-f1b6478b152e"
      },
      "source": [
        "response"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "outputs {\n",
              "  key: \"dense_1\"\n",
              "  value {\n",
              "    dtype: DT_FLOAT\n",
              "    tensor_shape {\n",
              "      dim {\n",
              "        size: 3\n",
              "      }\n",
              "      dim {\n",
              "        size: 10\n",
              "      }\n",
              "    }\n",
              "    float_val: 0.00011425150296417996\n",
              "    float_val: 1.5136735953547031e-07\n",
              "    float_val: 0.0009818377438932657\n",
              "    float_val: 0.0027773668989539146\n",
              "    float_val: 3.7589061321341433e-06\n",
              "    float_val: 7.62673225835897e-05\n",
              "    float_val: 3.9139663954301795e-08\n",
              "    float_val: 0.995561957359314\n",
              "    float_val: 5.344559394870885e-05\n",
              "    float_val: 0.000430887594120577\n",
              "    float_val: 0.0008195420377887785\n",
              "    float_val: 3.5499859222909436e-05\n",
              "    float_val: 0.9882416129112244\n",
              "    float_val: 0.00705768121406436\n",
              "    float_val: 1.2937896087805711e-07\n",
              "    float_val: 0.00023403267550747842\n",
              "    float_val: 0.002574468730017543\n",
              "    float_val: 9.668648104366184e-10\n",
              "    float_val: 0.0010369742522016168\n",
              "    float_val: 8.834076936636848e-08\n",
              "    float_val: 4.441462806425989e-05\n",
              "    float_val: 0.970328688621521\n",
              "    float_val: 0.009044435806572437\n",
              "    float_val: 0.0022598973009735346\n",
              "    float_val: 0.0004867135139647871\n",
              "    float_val: 0.0028736277017742395\n",
              "    float_val: 0.0022682773414999247\n",
              "    float_val: 0.008354853838682175\n",
              "    float_val: 0.004041310865432024\n",
              "    float_val: 0.0002978215052280575\n",
              "  }\n",
              "}\n",
              "model_spec {\n",
              "  name: \"my_mnist_model\"\n",
              "  version {\n",
              "    value: 1\n",
              "  }\n",
              "  signature_name: \"serving_default\"\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GxK7ZZ8qXcmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "dd00443d-793b-4db8-8e2d-fac3e92bd101"
      },
      "source": [
        "output_name = model.output_names[0]\n",
        "outputs_proto = response.outputs[output_name]\n",
        "y_proba = tf.make_ndarray(outputs_proto)\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlNs_YpKXcmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "be6021b8-244e-4388-8606-6e58d46636e6"
      },
      "source": [
        "output_name = model.output_names[0]\n",
        "outputs_proto = response.outputs[output_name]\n",
        "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
        "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxC7UpMTXcmN",
        "colab_type": "text"
      },
      "source": [
        "### 새로운 model 의 배포 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zAYYnIboXcmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "337c8a3c-1419-4e7e-e719-44a8304ab007"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dense(50, activation=\"relu\"),\n",
        "    keras.layers.Dense(50, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7039 - accuracy: 0.8056 - val_loss: 0.3418 - val_accuracy: 0.9042\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3204 - accuracy: 0.9082 - val_loss: 0.2674 - val_accuracy: 0.9242\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2650 - accuracy: 0.9235 - val_loss: 0.2227 - val_accuracy: 0.9368\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2319 - accuracy: 0.9329 - val_loss: 0.2032 - val_accuracy: 0.9432\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2089 - accuracy: 0.9399 - val_loss: 0.1833 - val_accuracy: 0.9482\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1908 - accuracy: 0.9446 - val_loss: 0.1740 - val_accuracy: 0.9498\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1756 - accuracy: 0.9490 - val_loss: 0.1605 - val_accuracy: 0.9540\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1631 - accuracy: 0.9524 - val_loss: 0.1543 - val_accuracy: 0.9558\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1517 - accuracy: 0.9568 - val_loss: 0.1459 - val_accuracy: 0.9574\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1429 - accuracy: 0.9583 - val_loss: 0.1358 - val_accuracy: 0.9612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib_sbME-XcmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ac5742b-c753-4ae6-ea4b-ea9e0bfdcb36"
      },
      "source": [
        "model_version = \"0002\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "model_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my_mnist_model/0002'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP4rSAaAXcmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2ce5811-5f53-4187-cfbb-c87589e39fc0"
      },
      "source": [
        "tf.saved_model.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx3iiqRKLH0n",
        "colab_type": "text"
      },
      "source": [
        "- 신규 버젼의 모델(0002) 이 my_mnist_model 에 추가됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UFHFj4hXcmT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "2e0cb86c-e9d8-4971-cb26-38a1e3e549db"
      },
      "source": [
        "for root, dirs, files in os.walk(model_name):\n",
        "    indent = '    ' * root.count(os.sep)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    for filename in files:\n",
        "        print('{}{}'.format(indent + '    ', filename))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_mnist_model/\n",
            "    0001/\n",
            "        saved_model.pb\n",
            "        variables/\n",
            "            variables.data-00001-of-00002\n",
            "            variables.index\n",
            "            variables.data-00000-of-00002\n",
            "        assets/\n",
            "    0002/\n",
            "        saved_model.pb\n",
            "        variables/\n",
            "            variables.data-00001-of-00002\n",
            "            variables.index\n",
            "            variables.data-00000-of-00002\n",
            "        assets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX7TjPttXUNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "b3e2d608-7056-4cd4-dc79-3e86e6101d9e"
      },
      "source": [
        "!zip -r my_mnist_model.zip my_mnist_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: my_mnist_model/ (stored 0%)\n",
            "updating: my_mnist_model/0001/ (stored 0%)\n",
            "updating: my_mnist_model/0001/saved_model.pb (deflated 87%)\n",
            "updating: my_mnist_model/0001/variables/ (stored 0%)\n",
            "updating: my_mnist_model/0001/variables/variables.data-00001-of-00002 (deflated 8%)\n",
            "updating: my_mnist_model/0001/variables/variables.index (deflated 52%)\n",
            "updating: my_mnist_model/0001/variables/variables.data-00000-of-00002 (deflated 72%)\n",
            "updating: my_mnist_model/0001/assets/ (stored 0%)\n",
            "updating: my_mnist_model/0002/ (stored 0%)\n",
            "updating: my_mnist_model/0002/saved_model.pb (deflated 88%)\n",
            "updating: my_mnist_model/0002/variables/ (stored 0%)\n",
            "updating: my_mnist_model/0002/variables/variables.data-00001-of-00002 (deflated 8%)\n",
            "updating: my_mnist_model/0002/variables/variables.index (deflated 54%)\n",
            "updating: my_mnist_model/0002/variables/variables.data-00000-of-00002 (deflated 75%)\n",
            "updating: my_mnist_model/0002/assets/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uRuOWgnV6sT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "efbe59c1-166f-49f3-d639-a3700e0e6ce2"
      },
      "source": [
        "# colab 내의 파일 다운로드\n",
        "from google.colab import files\n",
        "files.download('my_mnist_model.zip') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dca77853-e01f-4d10-bf74-af91ffbf2441\", \"my_mnist_model.zip\", 472423)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8rwYRN3XcmW",
        "colab_type": "text"
      },
      "source": [
        "- 수 분 정도의 시간 이후 신규 모델이 적용됨\n",
        "\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00238.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymTx8QZVXcmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
        "            \n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status()\n",
        "response = response.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAUo2IXkXcmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f8ca5e0-6815-410f-f745-36d95d5a0b92"
      },
      "source": [
        "response.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['predictions'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XDGX4ThXcmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e240a288-267e-485b-e74c-1659add115df"
      },
      "source": [
        "y_proba = np.array(response[\"predictions\"])\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2cUqm_YXcme",
        "colab_type": "text"
      },
      "source": [
        "## GCP AI Platform 에서 예측 서비스 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Wh0lokYAag",
        "colab_type": "text"
      },
      "source": [
        "### 사전준비\n",
        "- 실습해 보기 위해서는 GCP 가입 필요(휴대폰 인증 및 카드정보 등록 필요)\n",
        "- 새 계정 하나 만들어서 free tier 쓰는 것 추천(요즘은 가입시 폰번호 입력 안 받는 듯함)\n",
        "\n",
        "### GCP AI 플랫폼 quickstart\n",
        "- 구글계정 로그인 및 google cloud console 접속 : https://console.cloud.google.com/\n",
        "- 좌측메뉴 > Storage > Browser\n",
        "- 모델 파일을 업로드할 GCS bucket 을 생성\n",
        "   - us-central1, europe-west4, asia-east1 추천 : AI Platform 에서 지원되는 region\n",
        "- 생성한 saved_model 을 GCS bucket 에 업로드(base path 기준으로)\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00183.jpg)\n",
        "\n",
        "- 좌측메뉴 > AI Platform > Models\n",
        "- 처음이면 Enable API 클릭\n",
        "- Create Model\n",
        "  - GCS Bucket 과 같은 region 선택 \n",
        "- Create version(5분 이상 소요) : 프레임워크, 하드웨어 등 세부 설정이 들어감\n",
        "  - Python version\n",
        "  - Ml framework(scikit, TF, XGBoost 등)\n",
        "  - Accelerator(GPU) 장비 종류 및 unit 단위\n",
        "  - GCS 경로(browse 가능)\n",
        "  - Autoscaling 여부 옵션\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LilgF0fxlHcO",
        "colab_type": "text"
      },
      "source": [
        "### 예측 서비스 사용하기\n",
        "- 본 예제에서는 google api python sdk 를 사용\n",
        "\n",
        "#### 서비스 전용 credential 생성\n",
        "- Public cloud 활용 시 root credential 은 권한범위가 넓어서 위험하고 잘 사용하지 않음\n",
        "- 따라서 권한이 제한된 서비스 계정을 생성하고 json credential 을 만들고 연동하여 사용하는 것이 보통\n",
        "\n",
        "#### Service account 추가\n",
        "- 좌측메뉴 > IAM & Admin > Service account > Create service account\n",
        "- 생성된 계정 클릭 > Add keys\n",
        "\n",
        "#### IAM\n",
        "- 좌측메뉴 > IAM & Admin > IAM > Permission > ADD\n",
        "- 위에서 생성한 서비스 계정 선택, Role 에 ML Engine Developer 추가 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1QYE6NQX_pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "77d5ec33-a137-4bd7-a635-7098d6f0ce75"
      },
      "source": [
        "#!wget -O cred.json \"https://drive.google.com/u/0/uc?id=1eDk-dUdOno3RjOsiE6WsJujuwJ8sUJKN&export=download\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-02 07:27:19--  https://drive.google.com/u/0/uc?id=1eDk-dUdOno3RjOsiE6WsJujuwJ8sUJKN&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.203.100, 172.217.203.102, 172.217.203.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fvp3epjlcskh1rf3ohvqjsdoegmbql4i/1596353175000/14905503537404824244/*/1eDk-dUdOno3RjOsiE6WsJujuwJ8sUJKN?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-02 07:27:19--  https://doc-0s-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fvp3epjlcskh1rf3ohvqjsdoegmbql4i/1596353175000/14905503537404824244/*/1eDk-dUdOno3RjOsiE6WsJujuwJ8sUJKN?e=download\n",
            "Resolving doc-0s-ak-docs.googleusercontent.com (doc-0s-ak-docs.googleusercontent.com)... 74.125.26.132, 2607:f8b0:400c:c04::84\n",
            "Connecting to doc-0s-ak-docs.googleusercontent.com (doc-0s-ak-docs.googleusercontent.com)|74.125.26.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2341 (2.3K) [application/json]\n",
            "Saving to: ‘cred.json’\n",
            "\n",
            "\rcred.json             0%[                    ]       0  --.-KB/s               \rcred.json           100%[===================>]   2.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-02 07:27:19 (84.2 MB/s) - ‘cred.json’ saved [2341/2341]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klyImSXPwEir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "534d52b4-bec3-41ba-d5d4-b071c40d9009"
      },
      "source": [
        "# ml engine sdk 최근 샘플 코드 : Use your deployed model > Sample prediction request\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"cred.json\"\n",
        "\n",
        "import googleapiclient.discovery\n",
        "def predict_json(project, region, model, instances, version=None):\n",
        "    \"\"\"Send json data to a deployed model for prediction.\n",
        "\n",
        "    Args:\n",
        "        project (str): project where the Cloud ML Engine Model is deployed.\n",
        "        region (str): regional endpoint to use; set to None for ml.googleapis.com\n",
        "        model (str): model name.\n",
        "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
        "            your deployed model expects as inputs. Values should be datatypes\n",
        "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
        "            convertible to tensors.\n",
        "        version: str, version of the model to target.\n",
        "    Returns:\n",
        "        Mapping[str: any]: dictionary of prediction results defined by the\n",
        "            model.\n",
        "    \"\"\"\n",
        "    # Create the ML Engine service object.\n",
        "    # To authenticate set the environment variable\n",
        "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
        "    prefix = \"{}-ml\".format(region) if region else \"ml\"\n",
        "    api_endpoint = \"https://{}.googleapis.com\".format(prefix)\n",
        "    client_options = dict(api_endpoint=api_endpoint)\n",
        "    service = googleapiclient.discovery.build(\n",
        "        'ml', 'v1', client_options=client_options)\n",
        "    name = 'projects/{}/models/{}'.format(project, model)\n",
        "\n",
        "    if version is not None:\n",
        "        name += '/versions/{}'.format(version)\n",
        "\n",
        "    response = service.projects().predict(\n",
        "        name=name,\n",
        "        body={'instances': instances}\n",
        "    ).execute()\n",
        "\n",
        "    if 'error' in response:\n",
        "        raise RuntimeError(response['error'])\n",
        "\n",
        "    return response['predictions']\n",
        "\n",
        "\n",
        "predict_json(\"profound-gantry-285109\", \"us-central1\", \"my_mnist_model\", X_new.tolist(), version=\"v0001\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.000113918402,\n",
              "  1.51716876e-07,\n",
              "  0.000975968898,\n",
              "  0.00278414763,\n",
              "  3.75585432e-06,\n",
              "  7.64351134e-05,\n",
              "  3.90252559e-08,\n",
              "  0.995564222,\n",
              "  5.29630342e-05,\n",
              "  0.000428568979],\n",
              " [0.000814454,\n",
              "  3.52761344e-05,\n",
              "  0.988261163,\n",
              "  0.00706663728,\n",
              "  1.2855574e-07,\n",
              "  0.000233507817,\n",
              "  0.00255494262,\n",
              "  9.62903202e-10,\n",
              "  0.00103384326,\n",
              "  8.75075301e-08],\n",
              " [4.43605459e-05,\n",
              "  0.970256,\n",
              "  0.00910014,\n",
              "  0.00227546599,\n",
              "  0.000487301499,\n",
              "  0.00287919072,\n",
              "  0.00227399077,\n",
              "  0.00836221408,\n",
              "  0.00402356684,\n",
              "  0.000297729333]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CiGSL7-8S5v",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOPKEqTHZMWy",
        "colab_type": "text"
      },
      "source": [
        "# 모바일 또는 임베드 장치에 모델 배포하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no_H1x5oZfCs",
        "colab_type": "text"
      },
      "source": [
        "## TF Lite\n",
        "- https://www.tensorflow.org/lite/convert\n",
        "- TF Lite converter 를 이용해여 모델을 축소\n",
        "- 실제 모델 그래프의 최적화도 진행 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rinEu94vZycs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
        "converter.optimizations=[tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()\n",
        "with open(\"converted_model.tflite\",\"wb\") as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhHcgbvlbTkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1310207-553e-4924-dd2c-7659c3ead6fc"
      },
      "source": [
        "!ls -al converted_model.tflite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 171264 Aug  1 23:48 converted_model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXPaX7zudJiJ",
        "colab_type": "text"
      },
      "source": [
        "## 모델 그래프 최적화의 정도 \n",
        "- 큰 모델의 tflite, pb 비교해보면 알 수 있음 : https://www.tensorflow.org/lite/guide/hosted_models\n",
        "- TF 모델 그래프 조회 툴 : https://lutzroeder.github.io/netron/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH02qwdjh7t-",
        "colab_type": "text"
      },
      "source": [
        "## Post-training quantization\n",
        "- 모델의 weight 자료구조의 bit size 를 축소하는 기법\n",
        "   - 32bit float > 8bit int\n",
        "- abs(weight) 의 최대값을 기준으로 -127 ~ 127(signed 8bit int) scalar space 에 매핑\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00188.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WMUcwzgnvkl",
        "colab_type": "text"
      },
      "source": [
        "## Quantization 된 모델의 사용\n",
        "- float 로 복원하여 사용 : RAM사용량, 실행속도 등에서 이득이 없음\n",
        "- activation 출력을 quantization 처리하여 전체 내부 연산을 정수화\n",
        "  - Edge TPU 등"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqwyD_J8bZJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
        "converter.optimizations=[tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()\n",
        "with open(\"converted_model_quantized.tflite\",\"wb\") as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA92B-Gdk2cS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "27945d49-b439-45de-f232-139ec04b7e01"
      },
      "source": [
        "!ls -al converted_model_quantized.tflite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 46264 Aug  2 00:30 converted_model_quantized.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q39-6axfXcmp",
        "colab_type": "text"
      },
      "source": [
        "# GPU 를 통한 계산 속도 향상\n",
        "- TF GPU 지원 : https://www.tensorflow.org/install/gpu?hl=ko\n",
        "   - Compute Capability >= 3.5\n",
        "   - CUDA = 10.1\n",
        "   - cuDNN SDK >= 7.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAjPFGIc1pM",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://image.slidesharecdn.com/intototensorflowarchitecturev2-170703174758/95/an-introduction-to-tensorflow-architecture-7-638.jpg?cb=1520314886)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vqtlb0a8cCx",
        "colab_type": "text"
      },
      "source": [
        "- GPU 활성화여부 및 메타 정보 반환 등의 커맨드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsj07sF7Xcmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_gpu_available() # 최소 1개 이상의 GPU 가 사용 가능하면 True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmzJjk8bXcms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLxGgV60Xcmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTAGRdbfdCMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah0_7TYmXcmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client.device_lib import list_local_devices\n",
        "\n",
        "devices = list_local_devices()\n",
        "devices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D63Y7EP6fD8i",
        "colab_type": "text"
      },
      "source": [
        "## Public cloud 의 GPU VM 이용하기\n",
        "- GCP 의 경우 기본적으로는 GPU VM 할당량이 0 으로 되어 잇음.\n",
        "  - 할당량 수정을 요청하여 승인을 받으면 승인받은 수만큼 GPU VM 을 사용할 수 잇음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2kCV5TZfFt-",
        "colab_type": "text"
      },
      "source": [
        "## Colab 이용하기\n",
        "- Standard : 무료\n",
        "- Pro : $9.9 / month\n",
        "  - High RAM 등의 추가 옵션 제공"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP3-vMWpe8Wv",
        "colab_type": "text"
      },
      "source": [
        "## GPU RAM 관리\n",
        "- 기본적으로 TF 는 실행시점에 가능한 모든 GPU 의 RAM 을 확보\n",
        "  - Memory fragmentation 을 방지하기 위함\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLC8nE5g2Xqo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Process 별 device직접 분배\n",
        "- 환경 변수를 통해 BUS_ID 지정\n",
        "\n",
        "```bash\n",
        "CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICE=0,1 python3 program1.py\n",
        "CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICE=3,2 python3 program2.py\n",
        "```\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00201.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Uo6T0b2YLe",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### TF Process 별 명시적 메모리 limit 값 지정\n",
        "\n",
        "```python\n",
        "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpu,\n",
        "        [ tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
        "```\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00202.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDabBPmG27Rb",
        "colab_type": "text"
      },
      "source": [
        "### 필요할 때에 매모리를 점진적으로 점유하는 옵션\n",
        "\n",
        "```python\n",
        "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbshtHcD3T7p",
        "colab_type": "text"
      },
      "source": [
        "### 물리 GPU 를 여러 가상 GPU 로 지정하는 전략\n",
        "\n",
        "```python\n",
        "physical_gpus=tf.config.experimental.list_physcal_devices(\"GPU\")\n",
        "tf.config.experimental.set_virtual_device_configuration(\n",
        "    physical_gpus[0],\n",
        "    [ tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),\n",
        "      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9hoTVNU_WCD",
        "colab_type": "text"
      },
      "source": [
        "## 디바이스에 연산과 변수 할당하기\n",
        "- 보통 데이터 전처리를 CPU, 신경망 연산은 GPU 를 사용하도록 내부 분배됨\n",
        "- 일반적 컴퓨터 아키텍쳐 기반에서, GPU 는 메인보드 BUS(다른 PCI Device 와 대역폭을 공유) 를 통해서만 CPU, RAM 과 통신할 수 있기 때문에 통신폭의 구조적 한계가 있음 \n",
        "  - 즉 GPU 로의 불필요한 데이터 전송을 최소화 하는 것이 좋음\n",
        "- 단일 GPU 의 RAM 용량확충은 실질적으로 불가능하므로(즉 비싼 자원임), 현재 런타임 step 에서 GPU 프로세싱에 불필요하다고 보이는 variable 은 최대한 CPU RAM 에 가지고 있는 것이 좋다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX5g3B-XLIs3",
        "colab_type": "text"
      },
      "source": [
        "### 프로그램 리소스의 하드웨어 할당상태 확인 \n",
        "\n",
        "- device 속성을 통해 실제 variable 이 배치된 위치를 확인\n",
        "```python\n",
        ">>> a = tf.Variable(42.0)\n",
        ">>> a.device\n",
        "'/job:localhost/replica:0/task:0/device:GPU:0'\n",
        ">>> b = tf.Variable (42)\n",
        ">>> b.device\n",
        "```\n",
        "\n",
        "- 명시적으로 특정 디바이스에 variable 할당\n",
        "```python\n",
        ">>> with tf.device(\"/cpu:0\"):\n",
        "...     c = tf.Variable(42.0\n",
        ")\n",
        "...\n",
        ">>> c.device\n",
        "'/job:localhost/replica:0/task:0/device:CPU:0'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evnrfWuO_tG7",
        "colab_type": "text"
      },
      "source": [
        "## 다중 장치에서 병렬 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w0VDeoWMBZT",
        "colab_type": "text"
      },
      "source": [
        "### TF 의 일반적인 그래프 계산 방식\n",
        "- CPU, GPU 로 DL 그래프를 분할해서 실행할 때,\n",
        "- CPU 의 evaluation queue 로 이동된 연산은\n",
        "   - inter-op pool 에서는 모든 CPU thread 공유가 필요한 연산 처리\n",
        "   - intra-op pool 에서는 inter-op 연산 중 병렬 연산 처리(최대 CPU core 수 만큼) 가능한 연산을 전달 받아 처리\n",
        "- GPU 의 evaluation queue 로 이동된 연산은\n",
        "   - 단순하게 순차 처리됨\n",
        "   - 내부적으로 CUDA, cuDNN 라이브러리에서 멀티스레딩 처리됨\n",
        "   - GPU hw spec 의 CUDA core 수치로 병렬연산의 척도를 확인할 수 잇음\n",
        "  \n",
        "\n",
        "### 예시\n",
        "- 하기의 A,B,C 는 leaf node\n",
        "  - A,B 는 inter-op 처리, A 는 병렬처리 가능한 연산이라 가정하면, intra-op 로 전달 및 처리\n",
        "  - C 는 GPU 에서 바로 처리\n",
        "- 하위 node 가 처리되면 상위 node 를 처리할 수 있음\n",
        "  - A,B > F\n",
        "  - C -> D,E\n",
        "  - A,B,D,E -> F\n",
        "\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00212.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAYkYrxbMZ4u",
        "colab_type": "text"
      },
      "source": [
        "## 분산 전략 샘플 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkJXeolRXcm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df7LwSkIXcm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    return keras.models.Sequential([\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
        "                            padding=\"same\", input_shape=[28, 28, 1]),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"), \n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(units=64, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(units=10, activation='softmax'),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a30jf__7Xcm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "model = create_model()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=10,\n",
        "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKMc1tMXXcm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# Change the default all-reduce algorithm:\n",
        "#distribution = tf.distribute.MirroredStrategy(\n",
        "#    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "\n",
        "# Specify the list of GPUs to use:\n",
        "#distribution = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
        "\n",
        "# Use the central storage strategy instead:\n",
        "#distribution = tf.distribute.experimental.CentralStorageStrategy()\n",
        "\n",
        "#resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "#distribution = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(lr=1e-2),\n",
        "                  metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeJLPRUjXcm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100 # must be divisible by the number of workers\n",
        "model.fit(X_train, y_train, epochs=10,\n",
        "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06R7dzngXcnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-5SApCoXcnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "K = keras.backend\n",
        "\n",
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    optimizer = keras.optimizers.SGD()\n",
        "\n",
        "with distribution.scope():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().batch(batch_size)\n",
        "    input_iterator = distribution.make_dataset_iterator(dataset)\n",
        "    \n",
        "@tf.function\n",
        "def train_step():\n",
        "    def step_fn(inputs):\n",
        "        X, y = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            Y_proba = model(X)\n",
        "            loss = K.sum(keras.losses.sparse_categorical_crossentropy(y, Y_proba)) / batch_size\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    per_replica_losses = distribution.experimental_run(step_fn, input_iterator)\n",
        "    mean_loss = distribution.reduce(tf.distribute.ReduceOp.SUM,\n",
        "                                    per_replica_losses, axis=None)\n",
        "    return mean_loss\n",
        "\n",
        "n_epochs = 10\n",
        "with distribution.scope():\n",
        "    input_iterator.initialize()\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
        "        for iteration in range(len(X_train) // batch_size):\n",
        "            print(\"\\rLoss: {:.3f}\".format(train_step().numpy()), end=\"\")\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ8PC-2bXcnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100 # must be divisible by the number of workers\n",
        "model.fit(X_train, y_train, epochs=10,\n",
        "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgLCopV-XcnJ",
        "colab_type": "text"
      },
      "source": [
        "# 다중 장치에서 모델 훈련하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtdpcXHxQfCF",
        "colab_type": "text"
      },
      "source": [
        "## 모델 병렬화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdFsyznus28j",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- DNN : 가로 / 세로 분할 두 경우 다 통신량이 많아서 병렬 성능이 좋지 않음\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00252.jpg)\n",
        "\n",
        "- CNN : 세로 분할 시 DNN 에 비해 적은 서버간 통신량과 병렬 이득을 볼 수 있음\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00215.jpg)\n",
        "\n",
        "- RNN : 이론상으로는 각 cell 이 복잡하기 때문에, 가로분할 시 통신량이 많더라도 분할하는게 이득이라지만, in practice 시 1 GPU 가 더 낫다고 함  \n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00217.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Ydj78Qya99",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 병렬화\n",
        "- 기본적으로 여러 복사된 모델에 데이터를 배치 단위로 분리하여 학습시키는 전략을 말함\n",
        "- weight 관리 전략으로 크게 Mirrored, Parameterserver 등이 존재 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u1F66McvWx7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Mirrored stretegy\n",
        "- https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy\n",
        "- 각각의 모델에서 gradient 를 구한 후, 서버 간 통신으로 **평균값을 취하여, 동일한 update 를 적용 하는 방식**으로 모든 모델들의 상태를 동일하게 유지\n",
        "- Allreduce 알고리즘으로 효율화\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00218.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fXS7Ytd5kon",
        "colab_type": "text"
      },
      "source": [
        "### Parameter server strategy\n",
        "- https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy\n",
        "- 모든 모델의 weight 업데이트를 일치화하는 부분은 동일하지만, weight update 시 동기 / 비동기 방식을 모두 사용할 수 있는 특징이 있음\n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00219.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuxjqezSK4DX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 동기 방식\n",
        "- 모든 분산 모델의 gradient 수집을 기다렷다가 한꺼번에 업데이트 하는 방식\n",
        "- 결과적으로 Mirrored 와 동일하게 됨\n",
        "- 어느 한 쪽의 모델 학습 실행이 느려지는 경우 bottleneck 이 될 수 잇음\n",
        "\n",
        "### 비동기 방식\n",
        "- 수집단계가 없이, 각각의 모델들에서 gradient 가 계산되는 시점에 즉시 parameter server 의 weight 를 업데이트\n",
        "- bottleneck 이 없이 머신 성능을 최대한 사용할 수 있는 장점\n",
        "- 특정 모델에서 실행지연이 일어날시, stale gradient 문제가 발생할 수 있음\n",
        "  - 특정 gradient 의 늦은 적용으로 전체적인 학습 방향성에 방해를 줄 수 잇음\n",
        "- stale gradient 문제의 완화법\n",
        "  - learning rate 감소\n",
        "  - stale gradient 를 버리거나 크기축소\n",
        "  - minibatch 크기를 조절\n",
        "  - 1개 모델만 사용하여 초기 몇번의 epoch 을 돌려(param server update 포함)놓음 (warmup). 학습 초기에 gradient 가 제각각일 확률이 크다고 가정하는 방식으로 보임.\n",
        "  \n",
        "### 연구 근황(?)  \n",
        "- 구글의 최근논문에서는, 일부의 복제모델을 통한 동기화 방식을 사용해보니, 모델 성능과 리소스 효율 두 가지를 합리적으로 끌어올릴 수 있엇다고 함. \n",
        "\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00227.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNa0eX01QqS7",
        "colab_type": "text"
      },
      "source": [
        "### 대역폭의 포화\n",
        "- 다수의 GPU RAM 또는 다수의 서버장비 간에 gradient 를 통신하게 되됨\n",
        " - 복제 모델의 수가 임계치만큼 늘어나게 되면, 결국 하드웨어 BUS 또는 네트워크 대역의 한계로 성능의 이점을 잃게 됨 \n",
        " - 대체로 작은 모델을 많은 데이터로 학습하는 경우에는, GRAM bandwidth 가 높고 성능이 좋은 GPU 단일 머신을 사용하는 것이 낫다고 한다.\n",
        "- 대규모의 dense model 성능 문제 해결을 위한 여러 연구가 진행되고 잇음\n",
        " - peer-to-peer 파라미터 서버, 손실모델 압축, 복제 모델간 조건부 통신을 통한 최적화 연구 등 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDdKKyjwS_dg",
        "colab_type": "text"
      },
      "source": [
        "## 분산 전략 API 를 사용한 대규모 훈련\n",
        "- 추상화된 전략 API 메서드를 제공\n",
        "\n",
        "```python\n",
        "distribution = tf.distribute.MirroredStrategy()\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(lr=1e-2),\n",
        "                  metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "-  분산 전략 세부 설정\n",
        "\n",
        "```python\n",
        "# all-reduce algorithm 변경\n",
        "distribution = tf.distribute.MirroredStrategy(\n",
        "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "# GPU 목록 지정\n",
        "distribution = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
        "# 중앙 파라미터 전략\n",
        "distribution = tf.distribute.experimental.CentralStorageStrategy()\n",
        "# TPU 환경\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "distribution = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQDGbzKErxe",
        "colab_type": "text"
      },
      "source": [
        "## 텐서플로 클러스터에서 모델 훈련\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-ci9OJWAqX",
        "colab_type": "text"
      },
      "source": [
        "### Tensorflow Cluster\n",
        "- TF 훈련 또는 실행을 병렬로 실행하고 통신하는 TF 프로세스 그룹\n",
        "- Cluster 의 각 TF 프로세스는 Task 또는 TF server 로 호칭\n",
        " - IP, Port, Type(Role/Job) 속성을 가짐\n",
        "\n",
        "#### Task 의 Type\n",
        "- worker :  일반적으로 하나 이상의 GPU가 있는 머신에서 계산을 수행\n",
        "- chief : 계산 수행 및 TensorBoard 로그 작성,체크 포인트 저장 등의 추가 작업 처리\n",
        " - 클러스터에는 단일한 chief 존재, 미지정 시 첫 Task가 chief\n",
        "- ps(parameter server) : 변수 값만 추적하며 일반적으로 CPU 전용 시스템에 존재\n",
        "- evaluator : evaluation 처리. 일반적으로 1 클러스터에 1 evaluator 가 존재\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7VoDhDRZQrJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### 클러스터 지정\n",
        "- 사용할 모든 Task를 선 정의\n",
        "```json\n",
        "cluster_spec = {\n",
        "    \"worker\" : [\n",
        "        \"machine-a.example.com:2222\" ,   # /job:worker/task:0\n",
        "        \"machine-b.example.com:2222\"    # /job:worker/task:1\n",
        "    ],\n",
        "    \"ps\" : [ \"machine-a.example.com:2221\" ] # /job:ps/task:0\n",
        "}\n",
        "```\n",
        "![대체 텍스트](http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/Image00286.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WQxFdLzXcnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": {\n",
        "        \"worker\": [\"my-work0.example.com:9876\", \"my-work1.example.com:9876\"],\n",
        "        \"ps\": [\"my-ps0.example.com:9876\"]\n",
        "    },\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
        "})\n",
        "print(\"TF_CONFIG='{}'\".format(os.environ[\"TF_CONFIG\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4OhQIUFXcnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
        "worker0 = tf.distribute.Server(resolver.cluster_spec(),\n",
        "                               job_name=resolver.task_type,\n",
        "                               task_index=resolver.task_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S07zMgAGXcnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    \"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"],\n",
        "    \"ps\": [\"127.0.0.1:9903\"]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAXw1etWXcnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#worker1 = tf.distribute.Server(cluster_spec, job_name=\"worker\", task_index=1)\n",
        "ps0 = tf.distribute.Server(cluster_spec, job_name=\"ps\", task_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3_8pyn_XcnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": {\n",
        "        \"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"],\n",
        "        \"ps\": [\"127.0.0.1:9903\"]\n",
        "    },\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 1}\n",
        "})\n",
        "print(repr(os.environ[\"TF_CONFIG\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpNNvTgbXcnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": {\n",
        "        \"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"],\n",
        "        \"ps\": [\"127.0.0.1:9903\"]\n",
        "    },\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 1}\n",
        "})\n",
        "#CUDA_VISIBLE_DEVICES=0 \n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(lr=1e-2),\n",
        "                  metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfTHVV5pXcnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# At the beginning of the program (restart the kernel before running this cell)\n",
        "distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full[..., np.newaxis] / 255.\n",
        "X_test = X_test[..., np.newaxis] / 255.\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_new = X_test[:3]\n",
        "\n",
        "n_workers = 2\n",
        "batch_size = 32 * n_workers\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train[..., np.newaxis], y_train)).repeat().batch(batch_size)\n",
        "    \n",
        "def create_model():\n",
        "    return keras.models.Sequential([\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
        "                            padding=\"same\", input_shape=[28, 28, 1]),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"), \n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(units=64, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(units=10, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(lr=1e-2),\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(dataset, steps_per_epoch=len(X_train)//batch_size, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwoQapAkXcnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter tuning\n",
        "\n",
        "# Only talk to ps server\n",
        "config_proto = tf.ConfigProto(device_filters=['/job:ps', '/job:worker/task:%d' % tf_config['task']['index']])\n",
        "config = tf.estimator.RunConfig(session_config=config_proto)\n",
        "# default since 1.10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4DGhQ0pXcnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strategy.num_replicas_in_sync"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1BfHJuqRwbR",
        "colab_type": "text"
      },
      "source": [
        "## GCP AI 플랫폼으로 대규모 훈련 실행하기\n",
        "- gcloud cli shell 를 통해 job 을 trigger 하는 방식\n",
        "- 소스 코드 및 런타임을 GCP VM 에 설치 하고 실행\n",
        "\n",
        "```bash\n",
        "$ gcloud ai-platform jobs submit training my_job_20190531_164700 \\\n",
        "    --region asia-southeast1 \\\n",
        "    --scale-tier PREMIUM_1 \\\n",
        "    --runtime-version 2.0 \\\n",
        "    --python-version 3.5 \\\n",
        "    --package-path /my_project/src/trainer \\\n",
        "    --module-name trainer.task \\\n",
        "    --staging-bucket gs://my-staging-bucket \\\n",
        "    --job-dir gs://my-mnist-model-bucket/trained_model \\\n",
        "    --my-extra-argument1 foo --my-extra-argument2 bar\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asdDLRiq9C9W",
        "colab_type": "text"
      },
      "source": [
        "# Reference for media resources\n",
        "- http://reader.epubee.com/books/mobile/db/db4791ccfa2bab56aa04efa5d23d9336/text00013.html"
      ]
    }
  ]
}